{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Import Pandas library \n",
    "import numpy as np # Import Numpy library\n",
    "  \n",
    "# File name: logistic_regression.py\n",
    "# Author: Addison Sears-Collins\n",
    "# Date created: 7/19/2019\n",
    "# Python version: 3.7\n",
    "# Description: Multi-class logistic regression using one-vs-all. \n",
    "  \n",
    "# Required Data Set Format for Disrete Class Values\n",
    "# Columns (0 through N)\n",
    "# 0: Instance ID\n",
    "# 1: Attribute 1 \n",
    "# 2: Attribute 2\n",
    "# 3: Attribute 3 \n",
    "# ...\n",
    "# N: Actual Class\n",
    "  \n",
    "# This program then adds 2 additional columns for the test set.\n",
    "# N + 1: Predicted Class\n",
    "# N + 2: Prediction Correct? (1 if yes, 0 if no)\n",
    " \n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        z: A real number\n",
    "    Returns: \n",
    "        1.0/(1 + np.exp(-z))\n",
    "    \"\"\"\n",
    "    return 1.0/(1 + np.exp(-z))\n",
    " \n",
    "def gradient_descent(training_set):\n",
    "    \"\"\"\n",
    "    Gradient descent for logistic regression. Follows method presented\n",
    "    in the textbook Introduction to Machine Learning 3rd Edition by     \n",
    "    Ethem Alpaydin (pg. 252)\n",
    " \n",
    "    Parameters:\n",
    "      training_set: The training instances as a Numpy array\n",
    "    Returns:\n",
    "      weights: The vector of weights, commonly called w or THETA\n",
    "    \"\"\"  \n",
    " \n",
    "    no_of_columns_training_set = training_set.shape[1]\n",
    "    no_of_rows_training_set = training_set.shape[0]\n",
    " \n",
    "    # Extract the attributes from the training set.\n",
    "    # x is still a 2d array\n",
    "    x = training_set[:,:(no_of_columns_training_set - 1)]\n",
    "    no_of_attributes = x.shape[1]\n",
    " \n",
    "    # Extract the classes from the training set.\n",
    "    # actual_class is a 1d array.\n",
    "    actual_class = training_set[:,(no_of_columns_training_set - 1)]\n",
    " \n",
    "    # Set a learning rate\n",
    "    LEARNING_RATE = 0.01\n",
    " \n",
    "    # Set the maximum number of iterations\n",
    "    MAX_ITER = 10000\n",
    " \n",
    "    # Set the iteration variable to 0\n",
    "    iter = 0\n",
    " \n",
    "    # Set a flag to determine if we have exceeded the maximum number of\n",
    "    # iterations\n",
    "    exceeded_max_iter = False\n",
    " \n",
    "    # Set the tolerance. When the euclidean norm of the gradient vector \n",
    "    # (i.e. magnitude of the changes in the weights) gets below this value, \n",
    "    # stop iterating through the while loop\n",
    "    GRAD_TOLERANCE = 0.001\n",
    "    norm_of_gradient = None\n",
    " \n",
    "    # Set a flag to determine if we have reached the minimum of the \n",
    "    # cost (i.e. error) function.\n",
    "    converged = False\n",
    " \n",
    "    # Create the weights vector with random floats between -0.01 and 0.01\n",
    "    # The number of weights is equal to the number of attributes\n",
    "    weights = np.random.uniform(-0.01,0.01,(no_of_attributes))\n",
    "    changes_in_weights = None\n",
    " \n",
    "    # Keep running the loop below until convergence on the minimum of the \n",
    "    # cost function or we exceed the max number of iterations\n",
    "    while(not(converged) and not(exceeded_max_iter)):\n",
    "         \n",
    "        # Initialize a weight change vector that stores the changes in \n",
    "        # the weights at each iteration\n",
    "        changes_in_weights = np.zeros(no_of_attributes)\n",
    " \n",
    "        # For each training instance\n",
    "        for inst in range(0, no_of_rows_training_set):\n",
    " \n",
    "            # Calculate weighted sum of the attributes for\n",
    "            # this instance\n",
    "            output = np.dot(weights, x[inst,:])\n",
    "                 \n",
    "            # Calculate the sigmoid of the weighted sum\n",
    "            # This y is the probability that this instance belongs\n",
    "            # to the positive class\n",
    "            y =  sigmoid(output)\n",
    " \n",
    "            # Calculate difference\n",
    "            difference = (actual_class[inst] - y)\n",
    " \n",
    "            # Multiply the difference by the attribute vector\n",
    "            product = np.multiply(x[inst,:], difference)\n",
    " \n",
    "            # For each attribute, update the weight changes \n",
    "            # i.e. the gradient vector\n",
    "            changes_in_weights = np.add(changes_in_weights,product)\n",
    "         \n",
    "        # Calculate the step size\n",
    "        step_size = np.multiply(changes_in_weights, LEARNING_RATE)\n",
    " \n",
    "        # Update the weights vector\n",
    "        weights = np.add(weights, step_size)\n",
    " \n",
    "        # Test to see if we have converged on the minimum of the error\n",
    "        # function\n",
    "        norm_of_gradient = np.linalg.norm(changes_in_weights)\n",
    " \n",
    "        if (norm_of_gradient < GRAD_TOLERANCE):\n",
    "            converged = True\n",
    " \n",
    "        # Update the number of iterations\n",
    "        iter += 1\n",
    " \n",
    "        # If we have exceeded the maximum number of iterations\n",
    "        if (iter > MAX_ITER):\n",
    "            exceeded_max_iter = True\n",
    " \n",
    "    #For debugging purposes\n",
    "    #print(\"Number of Iterations: \" + str(iter - 1))\n",
    "    #print(\"Norm of the gradient: \" + str(norm_of_gradient))\n",
    "    #print(changes_in_weights)\n",
    "    #print()\n",
    "    return weights\n",
    " \n",
    " \n",
    "def logistic_regression(training_set, test_set):\n",
    "    \"\"\"\n",
    "    Multi-class one-vs-all logistic regression\n",
    "    Parameters:\n",
    "      training_set: The training instances as a Pandas dataframe\n",
    "      test_set: The test instances as a Pandas dataframe\n",
    "    Returns:\n",
    "      accuracy: Classification accuracy as a decimal\n",
    "      predictions: Classifications of all the test instances as a \n",
    "        Pandas dataframe\n",
    "      weights_for_each_class: The weight vectors for each class (one-vs-all)\n",
    "      no_of_instances_test: The number of test instances\n",
    "    \"\"\"  \n",
    " \n",
    "    # Remove the instance ID column\n",
    "    training_set = training_set.drop(\n",
    "        training_set.columns[[0]], axis=1)\n",
    "    test_set = test_set.drop(\n",
    "        test_set.columns[[0]], axis=1)\n",
    " \n",
    "    # Make a list of the unique classes\n",
    "    list_of_unique_classes = pd.unique(training_set[\"Actual Class\"])\n",
    " \n",
    "    # Replace all the class values with numbers, starting from 0\n",
    "    # in both the test and training sets.\n",
    "    for cl in range(0, len(list_of_unique_classes)):\n",
    "        training_set[\"Actual Class\"].replace(\n",
    "            list_of_unique_classes[cl], cl ,inplace=True)\n",
    "        test_set[\"Actual Class\"].replace(\n",
    "            list_of_unique_classes[cl], cl ,inplace=True)\n",
    " \n",
    "    # Insert a column of 1s in column 0 of both the training\n",
    "    # and test sets. This is the bias and helps with gradient\n",
    "    # descent. (i.e. X0 = 1 for all instances)\n",
    "    training_set.insert(0, \"Bias\", 1)\n",
    "    test_set.insert(0, \"Bias\", 1)\n",
    " \n",
    "    # Convert dataframes to numpy arrays\n",
    "    np_training_set = training_set.values\n",
    "    np_test_set = test_set.values\n",
    " \n",
    "    # Add 2 additional columns to the testing dataframe\n",
    "    test_set = test_set.reindex(\n",
    "        columns=[*test_set.columns.tolist(\n",
    "        ), 'Predicted Class', 'Prediction Correct?'])\n",
    " \n",
    "    ############################# Training Phase ##############################\n",
    " \n",
    "    no_of_columns_training_set = np_training_set.shape[1]\n",
    "    no_of_rows_training_set = np_training_set.shape[0]\n",
    " \n",
    "    # Create and store a training set for each unique class\n",
    "    # to create separate binary classification\n",
    "    # problems\n",
    "    trainingsets = []\n",
    "    for cl in range(0, len(list_of_unique_classes)):\n",
    " \n",
    "        # Create a copy of the training set\n",
    "        temp = np.copy(np_training_set)\n",
    " \n",
    "        # This class becomes the positive class 1\n",
    "        # and all other classes become the negative class 0\n",
    "        for row in range(0, no_of_rows_training_set):\n",
    "            if (temp[row, (no_of_columns_training_set - 1)]) == cl:\n",
    "                temp[row, (no_of_columns_training_set - 1)] = 1\n",
    "            else:\n",
    "                temp[row, (no_of_columns_training_set - 1)] = 0\n",
    "         \n",
    "        # Add the new training set to the trainingsets list\n",
    "        trainingsets.append(temp)\n",
    " \n",
    "    # Calculate and store the weights for the training set\n",
    "    # of each class. Execute gradient descent on each training set\n",
    "    # in order to calculate the weights\n",
    "    weights_for_each_class = []\n",
    " \n",
    "    for cl in range(0, len(list_of_unique_classes)):\n",
    "        weights_for_this_class = gradient_descent(trainingsets[cl])\n",
    "        weights_for_each_class.append(weights_for_this_class)\n",
    " \n",
    "    # Used for debugging\n",
    "    #print(weights_for_each_class[0])\n",
    "    #print()\n",
    "    #print(weights_for_each_class[1])\n",
    "    #print()\n",
    "    #print(weights_for_each_class[2])\n",
    " \n",
    "    ########################### End of Training Phase #########################\n",
    " \n",
    "    ############################# Testing Phase ###############################\n",
    " \n",
    "    no_of_columns_test_set = np_test_set.shape[1]\n",
    "    no_of_rows_test_set = np_test_set.shape[0]\n",
    " \n",
    "    # Extract the attributes from the test set.\n",
    "    # x is still a 2d array\n",
    "    x = np_test_set[:,:(no_of_columns_test_set - 1)]\n",
    "    no_of_attributes = x.shape[1]\n",
    " \n",
    "    # Extract the classes from the test set.\n",
    "    # actual_class is a 1d array.\n",
    "    actual_class = np_test_set[:,(no_of_columns_test_set - 1)]\n",
    " \n",
    "    # Go through each row (instance) of the test data\n",
    "    for inst in range(0,  no_of_rows_test_set):\n",
    " \n",
    "        # Create a scorecard that keeps track of the probabilities of this\n",
    "        # instance being a part of each class\n",
    "        scorecard = []\n",
    " \n",
    "        # Calculate and store the probability for each class in the scorecard\n",
    "        for cl in range(0, len(list_of_unique_classes)):\n",
    " \n",
    "            # Calculate weighted sum of the attributes for\n",
    "            # this instance\n",
    "            output = np.dot(weights_for_each_class[cl], x[inst,:])\n",
    " \n",
    "            # Calculate the sigmoid of the weighted sum\n",
    "            # This is the probability that this instance belongs\n",
    "            # to the positive class\n",
    "            this_probability = sigmoid(output)\n",
    " \n",
    "            scorecard.append(this_probability)\n",
    " \n",
    "        most_likely_class = scorecard.index(max(scorecard))\n",
    " \n",
    "        # Store the value of the most likely class in the \"Predicted Class\" \n",
    "        # column of the test_set data frame\n",
    "        test_set.loc[inst, \"Predicted Class\"] = most_likely_class\n",
    " \n",
    "        # Update the 'Prediction Correct?' column of the test_set data frame\n",
    "        # 1 if correct, else 0\n",
    "        if test_set.loc[inst, \"Actual Class\"] == test_set.loc[\n",
    "            inst, \"Predicted Class\"]:\n",
    "            test_set.loc[inst, \"Prediction Correct?\"] = 1\n",
    "        else:\n",
    "            test_set.loc[inst, \"Prediction Correct?\"] = 0\n",
    " \n",
    "    # accuracy = (total correct predictions)/(total number of predictions)\n",
    "    accuracy = (test_set[\"Prediction Correct?\"].sum())/(len(test_set.index))\n",
    " \n",
    "    # Store the revamped dataframe\n",
    "    predictions = test_set\n",
    " \n",
    "    # Replace all the class values with the name of the class\n",
    "    for cl in range(0, len(list_of_unique_classes)):\n",
    "        predictions[\"Actual Class\"].replace(\n",
    "            cl, list_of_unique_classes[cl] ,inplace=True)\n",
    "        predictions[\"Predicted Class\"].replace(\n",
    "            cl, list_of_unique_classes[cl] ,inplace=True)\n",
    " \n",
    "    # Replace 1 with Yes and 0 with No in the 'Prediction \n",
    "    # Correct?' column\n",
    "    predictions['Prediction Correct?'] = predictions[\n",
    "        'Prediction Correct?'].map({1: \"Yes\", 0: \"No\"})\n",
    " \n",
    "    # Reformat the weights_for_each_class list of arrays\n",
    "    weights_for_each_class = pd.DataFrame(np.row_stack(weights_for_each_class))\n",
    "  \n",
    "    # Rename the row names\n",
    "    for cl in range(0, len(list_of_unique_classes)):\n",
    "        row_name = str(list_of_unique_classes[cl] + \" weights\")        \n",
    "        weights_for_each_class.rename(index={cl:row_name}, inplace=True)\n",
    " \n",
    "    # Get a list of the names of the attributes\n",
    "    training_set_names = list(training_set.columns.values)\n",
    "    training_set_names.pop() # Remove 'Actual Class'\n",
    " \n",
    "    # Rename the column names\n",
    "    for col in range(0, len(training_set_names)):\n",
    "        col_name = str(training_set_names[col])        \n",
    "        weights_for_each_class.rename(columns={col:col_name}, inplace=True)\n",
    " \n",
    "    # Record the number of test instances\n",
    "    no_of_instances_test = len(test_set.index)\n",
    " \n",
    "    # Return statement\n",
    "    return accuracy, predictions, weights_for_each_class, no_of_instances_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
